<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CLAD: Bias Mitigation</title>
    <link rel="icon" href="https://harsmac.github.io/assets/favicon.svg" type="image/svg+xml" />
    <link rel="stylesheet" href="https://harsmac.github.io/style.css">
</head>

<body>
    <!-- Flowing waves background and social icons -->
    <div class="wave-bg" aria-hidden="true"></div>
    <aside class="social-bar" aria-label="Social links">
        <a href="https://github.com/harsmac" aria-label="GitHub" target="_blank" rel="noreferrer">
            <svg width="22" height="22" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true">
                <path
                    d="M12 .5a12 12 0 0 0-3.8 23.4c.6.1.8-.3.8-.6v-2c-3.3.7-4-1.4-4-1.4-.6-1.4-1.3-1.8-1.3-1.8-1-.7.1-.7.1-.7 1.1.1 1.7 1.2 1.7 1.2 1 1.7 2.7 1.2 3.3.9.1-.8.4-1.2.7-1.5-2.6-.3-5.3-1.3-5.3-5.8 0-1.3.5-2.4 1.2-3.3-.1-.3-.5-1.6.1-3.3 0 0 1-.3 3.4 1.3 1-.3 2-.4 3-.4s2 .1 3 .4C18 5.4 19 5.7 19 5.7c.6 1.7.2 3 .1 3.3.8.9 1.2 2 1.2 3.3 0 4.5-2.7 5.5-5.3 5.8.4.3.8 1 .8 2v3c0 .3.2.7.8.6A12 12 0 0 0 12 .5z" />
            </svg>
        </a>
        <a href="https://www.linkedin.com/in/harshitha-machiraju" aria-label="LinkedIn" target="_blank"
            rel="noreferrer">
            <svg width="22" height="22" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true">
                <path
                    d="M4.98 3.5C4.98 4.88 3.86 6 2.5 6S0 4.88 0 3.5 1.12 1 2.5 1 4.98 2.12 4.98 3.5zM.5 8.5h4V23h-4V8.5zM8.5 8.5h3.8v2h.1c.5-1 1.8-2.1 3.8-2.1 4 0 4.8 2.6 4.8 6v8.6h-4V15c0-1.9 0-4.4-2.7-4.4-2.7 0-3.1 2.1-3.1 4.2V23h-4V8.5z" />
            </svg>
        </a>
        <a href="mailto:harshitha.acad@gmail.com" aria-label="Email">
            <svg width="22" height="22" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true">
                <path d="M12 13 2 6.76V18a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V6.76L12 13zm10-9H2l10 6 10-6z" />
            </svg>
        </a>
    </aside>

    <!-- Header -->
    <header class="site-header">
        <a class="brand" href="https://harsmac.github.io/">HM</a>
        <button class="menu-toggle" aria-label="Toggle navigation" aria-expanded="false">☰</button>
        <nav class="site-nav" data-collapsible>
            <a href="https://harsmac.github.io/">Home</a>
            <a href="#overview">Overview</a>
            <a href="#method">Method</a>
            <a href="#results">Results</a>
            <a href="#bias">Bias</a>
            <a href="#takeaways">Takeaways</a>
            <input id="theme-toggle" type="checkbox" class="theme-toggle-input" aria-label="Toggle dark mode" />
            <label for="theme-toggle" class="theme-toggle" title="Toggle theme" aria-hidden="true"></label>
        </nav>
    </header>

<main>
    <!-- Hero -->
    <section class="hero">
        <div class="hero-text">
            <h1>CLAD: Contrastive Learning for Bias Mitigation</h1>
            <p>A contrastive learning approach to reduce background bias in vision models, encouraging models to focus
                on
                objects rather than spurious context.</p>
        </div>
        <div class="hero-photo">
            <img src="assets/clad/intro_fig.png" alt="Illustration of background bias problem" class="figure">
        </div>
    </section>

    <!-- Overview -->
    <section class="section" id="overview">
        <h2>Project Overview</h2>
        <p>
            Deep CNNs often exploit <strong>backgrounds and textures</strong> instead of focusing on foreground objects,
            leading to <em>spurious correlations</em> and poor generalization. CLAD introduces a framework that treats
            background swaps as positives and shared backgrounds as negatives, guiding models to learn
            <strong>foreground-aware representations</strong>.
        </p>
        <img src="assets/clad/construct_samples.png" alt="Foreground-background sample construction in CLAD"
            class="figure">
    </section>

    <!-- Method -->
    <section class="section" id="method">
        <h2>Methodology</h2>
        <ol>
            <li><strong>Foreground extraction:</strong> Separate object from background using segmentation.</li>
            <li><strong>Positive pairs:</strong> Replace the background of the same object with a random class
                background.</li>
            <li><strong>Negative pairs:</strong> Keep similar backgrounds but vary the object, managed with a dictionary
                queue.</li>
            <li><strong>Training objective:</strong> Combine supervised classification loss with contrastive InfoNCE
                loss.</li>
        </ol>
        <img src="assets/clad/flow_chart.png" alt="CLAD training pipeline diagram" class="figure">
    </section>

    <!-- Results -->
    <section class="section" id="results">
        <h2>Results</h2>
        <p>
            CLAD and CLAD+ outperform baselines on the Background Challenge:
        </p>
        <ul>
            <li>+4.1% accuracy on MIXED-RAND dataset.</li>
            <li>Reduced BG-GAP, showing less reliance on spurious backgrounds.</li>
            <li>Negligible drop (<0.5%) on ORIGINAL ImageNet-9.</li>
        </ul>
        <img src="assets/clad/results.png" alt="CLAD results on Background Challenge benchmark" class="figure">
        <img src="assets/clad/ablation_lambda.png" alt="Ablation study on contrastive loss weight" class="figure">
    </section>

    <!-- Bias -->
    <section class="section" id="bias">
        <h2>Bias Analysis</h2>
        <p>
            Saliency maps show that CLAD-trained models <em>focus on the object</em> rather than background context
            (e.g., wolves vs snow). Feature similarity analysis confirms >90% reliance on foreground features.
        </p>
        <img src="assets/clad/saliency.png" alt="Saliency maps comparing baseline vs CLAD-trained models"
            class="figure">
    </section>

    <!-- Takeaways -->
    <section class="section" id="takeaways">
        <h2>Takeaways</h2>
        <ul>
            <li>CLAD provides a scalable, efficient way to mitigate background bias.</li>
            <li>Contrastive learning balances robustness and accuracy without heavy augmentation.</li>
            <li>The approach generalizes to other biases, such as <strong>texture vs shape</strong>.</li>
        </ul>
    </section>

    <!-- Back link -->
    <div class="section" style="text-align:center;">
        <a href="index.html" class="primary-btn">← Back to Home</a>
    </div>
</main>


    <!-- Parallax scroll script -->
    <script src="https://harsmac.github.io/parallax.js"></script>
    <script src="https://harsmac.github.io/menu.js"></script>
    <script src="https://harsmac.github.io/theme.js"></script>

    <footer>
        <p>© 2025 Harshitha Machiraju. All rights reserved.</p>
    </footer>
</body>

</html>