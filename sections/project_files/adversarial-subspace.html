<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Adversarial Subspace Analysis in NLP</title>
    <link rel="stylesheet" href="https://harsmac.github.io/style.css">
</head>

<body>
    <!-- Flowing waves background and social icons -->
    <div class="wave-bg" aria-hidden="true"></div>
    <aside class="social-bar" aria-label="Social links">
        <a href="https://github.com/harsmac" aria-label="GitHub" target="_blank" rel="noreferrer">GH</a>
        <a href="https://www.linkedin.com/in/harshitha-machiraju" aria-label="LinkedIn" target="_blank"
            rel="noreferrer">IN</a>
        <a href="mailto:harshitha.acad@gmail.com" aria-label="Email">✉</a>
    </aside>

    <!-- Header -->
    <header class="site-header">
        <a class="brand" href="https://harsmac.github.io/">HM</a>
        <nav class="site-nav">
            <a href="https://harsmac.github.io/">Home</a>
            <a href="#overview">Overview</a>
            <a href="#method">Method</a>
            <a href="#results">Results</a>
            <a href="#bias">Bias</a>
            <a href="#takeaways">Takeaways</a>
        </nav>
    </header>

    <main>
        <!-- Hero -->
        <section class="hero">
            <h1>Adversarial Subspace Analysis in NLP</h1>
            <p>Exploring how adversarial perturbations reveal discriminative features and hidden biases in language models.</p>
            <img src="assets/adversarial_nlp/abstract-waves.svg" alt="" class="hero-art" aria-hidden="true">
        </section>


        <!-- Overview -->
        <section class="section" id="overview">
            <h2>Project Overview</h2>
            <p>
                Inspired by findings in vision, this project investigates whether adversarial examples also correspond
                to meaningful features in <strong>NLP models</strong>. Using BERT trained on IMDB sentiment analysis, we
                analyzed adversarial perturbations in the embedding space. We found they span a <em>low-dimensional
                    subspace</em>
                that captures most of the model’s discriminative power — but also align with <strong>bias-related
                    features</strong>
                such as gender and religion.
            </p>
            <img src="assets/adversarial_nlp/project-overview-abstract.png" alt="Abstract illustration of embedding space"
                class="decor-img">
        </section>

        <!-- Method -->
        <section class="section" id="method">
            <h2>Methodology</h2>
            <ol>
                <li>Generate adversarial perturbations in the embedding space (PGD attack).</li>
                <li>Perform Singular Value Decomposition (SVD) to identify dominant eigen-directions.</li>
                <li>Project embeddings onto these subspaces and evaluate classification accuracy.</li>
                <li>Probe how sensitive features (gender, religion) align with adversarial subspaces.</li>
            </ol>
            <img src="assets/adversarial_nlp/method-abstract.svg" alt="Geometric abstract diagram" class="decor-img">
        </section>

        <!-- Results -->
        <section class="section" id="results">
            <h2>Results</h2>
            <div class="decor-wrapper">
                <!-- <img src="assets/adversarial_nlp/results-spectrum.png" alt="Singular value spectrum" class="figure"> -->
                <img src="assets/adversarial_nlp/abstract-blob.svg" class="decor-img" alt="" aria-hidden="true">
            </div>
            <p class="caption">Singular value spectrum of adversarial perturbations (Standard vs Robust BERT).</p>
        </section>


        <!-- Bias -->
        <section class="section" id="bias">
            <h2>Bias Analysis</h2>
            <p>
                When projecting bias-related word pairs (e.g., <em>he/she</em>, <em>church/mosque</em>) into the
                adversarial
                basis, we found strong alignment with top components — meaning models treat these biases as
                discriminative. Robust training <em>did not</em> remove this effect.
            </p>
            <img src="assets/adversarial_nlp/bias-energy.png" alt="Bias decomposition energy plots" class="figure">
            <p class="caption">Energy decomposition of gender, religion, and subject biases in adversarial subspaces.
            </p>
        </section>

        <!-- Takeaways -->
        <section class="section" id="takeaways">
            <h2>What I Learned</h2>
            <ul>
                <li>Adversarial perturbations reveal compact, high-value subspaces in embeddings.</li>
                <li>Robustness ≠ Fairness — adversarial training alone does not mitigate bias.</li>
                <li>Geometric analysis (SVD of perturbations) is a powerful tool for model interpretability.</li>
            </ul>
            <p>
                Future work includes semantically-preserving attacks, dataset-level bias swaps, and applying
                this framework to <strong>larger LLMs</strong>.
            </p>
        </section>

        <!-- Back link -->
        <div class="section" style="text-align:center;">
            <a href="index.html" class="primary-btn">← Back to Home</a>
        </div>
    </main>

    <footer>
        <p>© 2025 Harshitha Machiraju. All rights reserved.</p>
    </footer>
</body>

</html>